{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishek0697/Detection-of-Hate-Speech-in-Multimodal-Memes/blob/main/Code/Experiments/BERT%2BResNeXt%2BImage_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fNxB0trzaP8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "001cecf2-689a-460e-c060-9710b7152d33"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 10 11:52:39 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8              12W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZTsHt3ON9-3",
        "outputId": "eee3e14f-bb12-4730-b167-2be5cf34595f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "l5rwo5ktODoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HateMemeDataset(Dataset):\n",
        "    def __init__(self, labels_file,textfile, img_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            labels_file (string): Path to the text file with annotations.\n",
        "            img_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on an image.\n",
        "        \"\"\"\n",
        "        self.img_labels = []\n",
        "        with open(labels_file, 'r') as file:\n",
        "            for line in file:\n",
        "                filename, label = line.strip().split('\\t')\n",
        "                # Convert label from string to integer (0 for non_hate, 1 for hate)\n",
        "                label_int = 0 if label == 'non_hate' else 1\n",
        "                self.img_labels.append((filename, label_int))\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels[idx][0])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.img_labels[idx][1]\n",
        "        text = self.img_labels[idx][1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, text, label\n"
      ],
      "metadata": {
        "id": "6W5MU8t4OJvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "dKbsKpPnOP-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = HateMemeDataset(labels_file='label.txt',textfile='/content/csv - Sheet1.csv', img_dir='/content/drive/MyDrive/trainlist', transform=transform)\n"
      ],
      "metadata": {
        "id": "uPiB3d2lOTRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into train and validation\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n"
      ],
      "metadata": {
        "id": "uKnzK-noOqda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "cVgn-bb2OyG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have the train_dataset and train_loader already defined\n",
        "dataset_size = len(train_dataset)  # Total number of items in your dataset\n",
        "batch_size = train_loader.batch_size  # Batch size you're using for the DataLoader\n",
        "\n",
        "# Calculate the number of batches, rounding up in case of a non-integer result\n",
        "import math\n",
        "num_batches = math.ceil(dataset_size / batch_size)\n",
        "\n",
        "print(f\"Dataset size: {dataset_size}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Number of batches in train_loader: {num_batches}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrLZ_HiDP5Lt",
        "outputId": "86b7b6bb-35b5-4f93-fd11-0ab27fdf812a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 16\n",
            "Batch size: 1\n",
            "Number of batches in train_loader: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAbMlndnaP8k"
      },
      "source": [
        "'''\n",
        "IMPORTING NECESSARY MODULES\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHfFe7WDaP8k"
      },
      "source": [
        "**Device**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFeNpEoAaP8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae14236-1939-4c4a-8b09-a76d5c77e251"
      },
      "source": [
        "gpu_ids = [7,6]\n",
        "device = torch.device('cuda:'+ str(gpu_ids[0]) if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFqdxztfaP8m"
      },
      "source": [
        "**Model Definition**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF0xzeO-23aw",
        "outputId": "02965f22-769a-45ec-c4d2-6e8b11a7cac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7, 6]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SPwyK-iK3JQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install torch"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDk984qh3J8b",
        "outputId": "3c3bdcb9-f8da-413f-959c-8bede9e38ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "0RNEfnMV3K1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_0Ul7UGW3qcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = BertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "# Load pre-trained ResNet model\n",
        "resnet_model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Define preprocessing transforms for ResNet\n",
        "preprocess_resnet = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Define preprocessing transforms for BERT\n",
        "def preprocess_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    return inputs\n",
        "\n",
        "# Function to extract text features from BERT\n",
        "def extract_text_features(text):\n",
        "    inputs = preprocess_text(text)\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "    text_features = outputs.last_hidden_state.mean(dim=1)  # Use mean pooling over tokens\n",
        "    return text_features\n",
        "\n",
        "# Function to extract image features from ResNet\n",
        "def extract_image_features(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    input_tensor = preprocess_resnet(image)\n",
        "    input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        image_features = resnet_model(input_batch)\n",
        "    image_features = image_features.flatten()  # Flatten the features\n",
        "    return image_features\n",
        "\n"
      ],
      "metadata": {
        "id": "5oKG1-M97V9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85pRXktUaP8o"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FusionNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, drop_prob=0.1):\n",
        "        super(FusionNet, self).__init__()\n",
        "\n",
        "        self.concat = nn.Linear(in_features=768+2048+768, out_features=512)\n",
        "        self.bn = nn.BatchNorm1d(512)\n",
        "        self.bn1 = nn.BatchNorm1d(768)\n",
        "        self.bn2 = nn.BatchNorm1d(2048)\n",
        "        self.bn3 = nn.BatchNorm1d(768)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.classify = nn.Linear(in_features=512, out_features=num_classes)\n",
        "\n",
        "    def forward(self, text_features, image_features, caption_features):\n",
        "\n",
        "        text_features = self.bn1(text_features)\n",
        "        image_features = self.bn2(image_features)\n",
        "        caption_features = self.bn3(caption_features)\n",
        "\n",
        "        # Concatenate the features\n",
        "        fused_input = torch.cat((text_features, image_features, caption_features), dim=1)\n",
        "\n",
        "        # Pass through linear layer and apply batch normalization\n",
        "        x = self.concat(fused_input)\n",
        "        x = self.bn(x)\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply ReLU activation function\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Classification layer\n",
        "        x = self.classify(x)\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print"
      ],
      "metadata": {
        "id": "8PiTJkkU8tAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "print(len(train_loader))\n",
        "print(train_loader.dataset)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hchVzfWv8v0k",
        "outputId": "32ab8682-5d6e-4f38-b8e8-95dfc20e79d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "<torch.utils.data.dataset.Subset object at 0x781c58c2afe0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    # Iterate over training dataset in batches\n",
        "    for batch in train_loader:\n",
        "        # Extract image and text data from the batch\n",
        "        images, texts, labels = batch\n",
        "\n",
        "        # Preprocess the image and text data\n",
        "        processed_images = preprocess_images(images)\n",
        "        processed_texts = preprocess_texts(texts)\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = fusion_model(processed_images, processed_texts)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_function(predictions, labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "0mb8zRLT8U-O",
        "outputId": "471dfedd-eaf7-43aa-deb0-775fab91aa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocess_images' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-946bab361dff>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Preprocess the image and text data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mprocessed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprocessed_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocess_images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdt6fL6-aP8o"
      },
      "source": [
        "**Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC_FZ5uCaP8o"
      },
      "source": [
        "'''\n",
        "Loss Function\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "Optimizer\n",
        "'''\n",
        "optimizer = torch.optim.SGD(Fusion_model.parameters(), lr=0.1, weight_decay=1e-4, momentum=0.9)\n",
        "# optimizer = AdamW(Fusion_model.parameters(), lr = 2e-3, eps = 1e-8)\n",
        "\n",
        "\n",
        "'''\n",
        "Number of training epochs.\n",
        "'''\n",
        "num_Epochs = 10\n",
        "\n",
        "\n",
        "# '''\n",
        "# OneCycleLR\n",
        "# '''\n",
        "# max_lr = 0.05\n",
        "# lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, total_steps=None, epochs=num_Epochs, steps_per_epoch=len(train_dataloader), pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, last_epoch=-1)\n",
        "\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 4, gamma = 0.001)\n",
        "\n",
        "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agClo63KaP8p"
      },
      "source": [
        "model_name = 'ImageCaptioning'\n",
        "model_path = '.saved_model_checkpoints/'+model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJUzH2BIaP8p"
      },
      "source": [
        "writer = SummaryWriter(model_name)\n",
        "\n",
        "train(Image_model, Text_model, Fusion_model, train_dataloader, validation_dataloader, criterion, optimizer, lr_scheduler, model_path, writer, device, epochs = num_Epochs)\n",
        "\n",
        "writer.flush()\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL0-sKbkaP8p"
      },
      "source": [
        "'''\n",
        "Load saved model from checkpoint\n",
        "'''\n",
        "Fusion_model, optimizer, lr_scheduler, train_loss, v_loss, v_acc, epoch = load(model_path, Fusion_model, optimizer, lr_scheduler)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZY-h3UtaP8q"
      },
      "source": [
        "**Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LZVVQ-1aP8q"
      },
      "source": [
        "test_classify(Fusion_model, validation_dataloader, criterion, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzTSBs4SaP8q"
      },
      "source": [
        "**Predict on Test and generate output.csv**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQM4UyDkaP8r"
      },
      "source": [
        "**Test Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHDvADPBaP8r"
      },
      "source": [
        "testlist = 'path of test set'\n",
        "\n",
        "test_dataset = mytestdataset(testlist, name='test')\n",
        "test_dataloader = data.DataLoader(test_dataset, shuffle= False, batch_size = 32, num_workers=8,pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPoGZ1L0aP8r"
      },
      "source": [
        "from predict import predict\n",
        "predict(image_model, text_model, fusion_model, test_dataloader, device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}